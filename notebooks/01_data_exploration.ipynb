{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Brazilian E-commerce Dataset - Initial Data Exploration\n",
                "\n",
                "This notebook provides a comprehensive exploration of the Brazilian E-commerce dataset, including:\n",
                "- Data loading and validation\n",
                "- Data quality assessment\n",
                "- Basic statistical analysis\n",
                "- Relationship analysis between datasets\n",
                "- Initial findings and insights"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import required libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import warnings\n",
                "import sys\n",
                "import os\n",
                "\n",
                "# Add parent directory to path to import our data_loader module\n",
                "sys.path.append('..')\n",
                "from data_loader import DataLoader, load_brazilian_ecommerce_data\n",
                "\n",
                "# Configure display options\n",
                "pd.set_option('display.max_columns', None)\n",
                "pd.set_option('display.max_rows', 100)\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set up plotting style\n",
                "plt.style.use('default')\n",
                "sns.set_palette(\"husl\")\n",
                "plt.rcParams['figure.figsize'] = (12, 8)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Loading and Initial Overview"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load all datasets using our data loader\n",
                "print(\"Loading Brazilian E-commerce datasets...\")\n",
                "datasets, summary = load_brazilian_ecommerce_data()\n",
                "\n",
                "print(f\"\\nSuccessfully loaded {len(datasets)} datasets\")\n",
                "print(\"\\nDataset Summary:\")\n",
                "display(summary)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display basic information about each dataset\n",
                "for name, df in datasets.items():\n",
                "    print(f\"\\n{'='*50}\")\n",
                "    print(f\"DATASET: {name.upper()}\")\n",
                "    print(f\"{'='*50}\")\n",
                "    print(f\"Shape: {df.shape}\")\n",
                "    print(f\"\\nColumns: {list(df.columns)}\")\n",
                "    print(f\"\\nData Types:\")\n",
                "    print(df.dtypes)\n",
                "    print(f\"\\nFirst 3 rows:\")\n",
                "    display(df.head(3))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Data Quality Assessment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Comprehensive missing value analysis\n",
                "def analyze_missing_values(datasets):\n",
                "    missing_analysis = []\n",
                "    \n",
                "    for name, df in datasets.items():\n",
                "        missing_counts = df.isnull().sum()\n",
                "        missing_percentages = (missing_counts / len(df)) * 100\n",
                "        \n",
                "        for column in df.columns:\n",
                "            if missing_counts[column] > 0:\n",
                "                missing_analysis.append({\n",
                "                    'Dataset': name,\n",
                "                    'Column': column,\n",
                "                    'Missing_Count': missing_counts[column],\n",
                "                    'Missing_Percentage': round(missing_percentages[column], 2),\n",
                "                    'Total_Rows': len(df)\n",
                "                })\n",
                "    \n",
                "    return pd.DataFrame(missing_analysis).sort_values('Missing_Percentage', ascending=False)\n",
                "\n",
                "missing_df = analyze_missing_values(datasets)\n",
                "print(\"Missing Values Analysis:\")\n",
                "display(missing_df)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize missing values\n",
                "if not missing_df.empty:\n",
                "    plt.figure(figsize=(14, 8))\n",
                "    \n",
                "    # Create a bar plot of missing percentages\n",
                "    missing_plot_data = missing_df.head(15)  # Top 15 columns with missing values\n",
                "    \n",
                "    bars = plt.bar(range(len(missing_plot_data)), missing_plot_data['Missing_Percentage'])\n",
                "    plt.xlabel('Dataset.Column')\n",
                "    plt.ylabel('Missing Percentage (%)')\n",
                "    plt.title('Top 15 Columns with Missing Values')\n",
                "    plt.xticks(range(len(missing_plot_data)), \n",
                "               [f\"{row['Dataset']}.{row['Column']}\" for _, row in missing_plot_data.iterrows()], \n",
                "               rotation=45, ha='right')\n",
                "    \n",
                "    # Add value labels on bars\n",
                "    for i, bar in enumerate(bars):\n",
                "        height = bar.get_height()\n",
                "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
                "                f'{height:.1f}%', ha='center', va='bottom')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "else:\n",
                "    print(\"No missing values found in any dataset!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Duplicate analysis\n",
                "def analyze_duplicates(datasets):\n",
                "    duplicate_analysis = []\n",
                "    \n",
                "    for name, df in datasets.items():\n",
                "        total_rows = len(df)\n",
                "        duplicate_rows = df.duplicated().sum()\n",
                "        duplicate_percentage = (duplicate_rows / total_rows) * 100 if total_rows > 0 else 0\n",
                "        \n",
                "        duplicate_analysis.append({\n",
                "            'Dataset': name,\n",
                "            'Total_Rows': total_rows,\n",
                "            'Duplicate_Rows': duplicate_rows,\n",
                "            'Duplicate_Percentage': round(duplicate_percentage, 2),\n",
                "            'Unique_Rows': total_rows - duplicate_rows\n",
                "        })\n",
                "    \n",
                "    return pd.DataFrame(duplicate_analysis).sort_values('Duplicate_Percentage', ascending=False)\n",
                "\n",
                "duplicates_df = analyze_duplicates(datasets)\n",
                "print(\"Duplicate Analysis:\")\n",
                "display(duplicates_df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Dataset Relationships and Foreign Key Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze relationships between datasets\n",
                "loader = DataLoader()\n",
                "loader.datasets = datasets\n",
                "relationships = loader.validate_data_relationships()\n",
                "\n",
                "print(\"Dataset Relationship Analysis:\")\n",
                "for relationship, details in relationships.items():\n",
                "    print(f\"\\n{relationship.upper().replace('_', ' ')}:\")\n",
                "    for key, value in details.items():\n",
                "        print(f\"  {key.replace('_', ' ').title()}: {value:,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze unique identifiers in each dataset\n",
                "def analyze_unique_identifiers(datasets):\n",
                "    identifier_analysis = []\n",
                "    \n",
                "    # Define potential identifier columns for each dataset\n",
                "    identifier_columns = {\n",
                "        'customers': ['customer_id', 'customer_unique_id'],\n",
                "        'orders': ['order_id'],\n",
                "        'order_items': ['order_id', 'order_item_id'],\n",
                "        'order_payments': ['order_id'],\n",
                "        'order_reviews': ['review_id', 'order_id'],\n",
                "        'products': ['product_id'],\n",
                "        'sellers': ['seller_id'],\n",
                "        'geolocation': ['geolocation_zip_code_prefix'],\n",
                "        'product_categories': ['product_category_name']\n",
                "    }\n",
                "    \n",
                "    for dataset_name, df in datasets.items():\n",
                "        if dataset_name in identifier_columns:\n",
                "            for col in identifier_columns[dataset_name]:\n",
                "                if col in df.columns:\n",
                "                    total_rows = len(df)\n",
                "                    unique_values = df[col].nunique()\n",
                "                    null_values = df[col].isnull().sum()\n",
                "                    \n",
                "                    identifier_analysis.append({\n",
                "                        'Dataset': dataset_name,\n",
                "                        'Column': col,\n",
                "                        'Total_Rows': total_rows,\n",
                "                        'Unique_Values': unique_values,\n",
                "                        'Null_Values': null_values,\n",
                "                        'Uniqueness_Ratio': round(unique_values / (total_rows - null_values), 4) if (total_rows - null_values) > 0 else 0\n",
                "                    })\n",
                "    \n",
                "    return pd.DataFrame(identifier_analysis)\n",
                "\n",
                "identifiers_df = analyze_unique_identifiers(datasets)\n",
                "print(\"Unique Identifier Analysis:\")\n",
                "display(identifiers_df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Basic Statistical Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze numerical columns across all datasets\n",
                "def analyze_numerical_columns(datasets):\n",
                "    numerical_analysis = []\n",
                "    \n",
                "    for name, df in datasets.items():\n",
                "        numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
                "        \n",
                "        for col in numerical_cols:\n",
                "            stats = df[col].describe()\n",
                "            numerical_analysis.append({\n",
                "                'Dataset': name,\n",
                "                'Column': col,\n",
                "                'Count': int(stats['count']),\n",
                "                'Mean': round(stats['mean'], 2),\n",
                "                'Std': round(stats['std'], 2),\n",
                "                'Min': stats['min'],\n",
                "                'Max': stats['max'],\n",
                "                'Median': round(stats['50%'], 2)\n",
                "            })\n",
                "    \n",
                "    return pd.DataFrame(numerical_analysis)\n",
                "\n",
                "numerical_df = analyze_numerical_columns(datasets)\n",
                "print(\"Numerical Columns Analysis:\")\n",
                "display(numerical_df)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze categorical columns\n",
                "def analyze_categorical_columns(datasets, max_unique_values=20):\n",
                "    categorical_analysis = []\n",
                "    \n",
                "    for name, df in datasets.items():\n",
                "        categorical_cols = df.select_dtypes(include=['object']).columns\n",
                "        \n",
                "        for col in categorical_cols:\n",
                "            unique_count = df[col].nunique()\n",
                "            if unique_count <= max_unique_values:  # Only analyze columns with reasonable number of categories\n",
                "                top_values = df[col].value_counts().head(5)\n",
                "                categorical_analysis.append({\n",
                "                    'Dataset': name,\n",
                "                    'Column': col,\n",
                "                    'Unique_Values': unique_count,\n",
                "                    'Most_Common': top_values.index[0] if len(top_values) > 0 else 'N/A',\n",
                "                    'Most_Common_Count': top_values.iloc[0] if len(top_values) > 0 else 0,\n",
                "                    'Top_5_Values': dict(top_values)\n",
                "                })\n",
                "    \n",
                "    return pd.DataFrame(categorical_analysis)\n",
                "\n",
                "categorical_df = analyze_categorical_columns(datasets)\n",
                "print(\"Categorical Columns Analysis (with â‰¤20 unique values):\")\n",
                "for _, row in categorical_df.iterrows():\n",
                "    print(f\"\\n{row['Dataset']}.{row['Column']}:\")\n",
                "    print(f\"  Unique values: {row['Unique_Values']}\")\n",
                "    print(f\"  Most common: {row['Most_Common']} ({row['Most_Common_Count']} occurrences)\")\n",
                "    print(f\"  Top 5 values: {row['Top_5_Values']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Date/Time Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Identify and analyze date columns\n",
                "def analyze_date_columns(datasets):\n",
                "    date_analysis = []\n",
                "    \n",
                "    for name, df in datasets.items():\n",
                "        # Look for columns that might contain dates\n",
                "        potential_date_cols = [col for col in df.columns if any(keyword in col.lower() \n",
                "                              for keyword in ['date', 'time', 'timestamp'])]\n",
                "        \n",
                "        for col in potential_date_cols:\n",
                "            # Try to convert to datetime and analyze\n",
                "            try:\n",
                "                # Sample a few values to check format\n",
                "                sample_values = df[col].dropna().head(10).tolist()\n",
                "                null_count = df[col].isnull().sum()\n",
                "                null_percentage = (null_count / len(df)) * 100\n",
                "                \n",
                "                date_analysis.append({\n",
                "                    'Dataset': name,\n",
                "                    'Column': col,\n",
                "                    'Null_Count': null_count,\n",
                "                    'Null_Percentage': round(null_percentage, 2),\n",
                "                    'Sample_Values': sample_values[:3],  # First 3 non-null values\n",
                "                    'Data_Type': str(df[col].dtype)\n",
                "                })\n",
                "            except Exception as e:\n",
                "                print(f\"Error analyzing {name}.{col}: {str(e)}\")\n",
                "    \n",
                "    return pd.DataFrame(date_analysis)\n",
                "\n",
                "date_df = analyze_date_columns(datasets)\n",
                "print(\"Date/Time Columns Analysis:\")\n",
                "display(date_df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Key Findings and Initial Insights"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate comprehensive data quality report\n",
                "def generate_data_quality_report(datasets, missing_df, duplicates_df, relationships):\n",
                "    report = []\n",
                "    report.append(\"=\" * 80)\n",
                "    report.append(\"BRAZILIAN E-COMMERCE DATASET - DATA QUALITY REPORT\")\n",
                "    report.append(\"=\" * 80)\n",
                "    \n",
                "    # Overall statistics\n",
                "    total_rows = sum(df.shape[0] for df in datasets.values())\n",
                "    total_columns = sum(df.shape[1] for df in datasets.values())\n",
                "    total_memory = sum(df.memory_usage(deep=True).sum() for df in datasets.values()) / 1024 / 1024\n",
                "    \n",
                "    report.append(f\"\\nðŸ“Š OVERALL STATISTICS:\")\n",
                "    report.append(f\"   â€¢ Total datasets: {len(datasets)}\")\n",
                "    report.append(f\"   â€¢ Total rows: {total_rows:,}\")\n",
                "    report.append(f\"   â€¢ Total columns: {total_columns}\")\n",
                "    report.append(f\"   â€¢ Total memory usage: {total_memory:.2f} MB\")\n",
                "    \n",
                "    # Data quality issues\n",
                "    report.append(f\"\\nðŸ” DATA QUALITY ISSUES:\")\n",
                "    \n",
                "    # Missing values\n",
                "    if not missing_df.empty:\n",
                "        high_missing = missing_df[missing_df['Missing_Percentage'] > 10]\n",
                "        report.append(f\"   â€¢ Columns with >10% missing values: {len(high_missing)}\")\n",
                "        if len(high_missing) > 0:\n",
                "            report.append(f\"     - Highest: {high_missing.iloc[0]['Dataset']}.{high_missing.iloc[0]['Column']} ({high_missing.iloc[0]['Missing_Percentage']:.1f}%)\")\n",
                "    else:\n",
                "        report.append(f\"   â€¢ No missing values found!\")\n",
                "    \n",
                "    # Duplicates\n",
                "    high_duplicates = duplicates_df[duplicates_df['Duplicate_Percentage'] > 5]\n",
                "    report.append(f\"   â€¢ Datasets with >5% duplicates: {len(high_duplicates)}\")\n",
                "    if len(high_duplicates) > 0:\n",
                "        report.append(f\"     - Highest: {high_duplicates.iloc[0]['Dataset']} ({high_duplicates.iloc[0]['Duplicate_Percentage']:.1f}%)\")\n",
                "    \n",
                "    # Relationship issues\n",
                "    report.append(f\"\\nðŸ”— RELATIONSHIP ANALYSIS:\")\n",
                "    for rel_name, rel_data in relationships.items():\n",
                "        report.append(f\"   â€¢ {rel_name.replace('_', ' ').title()}:\")\n",
                "        for key, value in rel_data.items():\n",
                "            report.append(f\"     - {key.replace('_', ' ').title()}: {value:,}\")\n",
                "    \n",
                "    # Recommendations\n",
                "    report.append(f\"\\nðŸ’¡ RECOMMENDATIONS:\")\n",
                "    report.append(f\"   1. Address high missing value columns (especially review comments)\")\n",
                "    report.append(f\"   2. Remove duplicate records from geolocation dataset\")\n",
                "    report.append(f\"   3. Investigate foreign key mismatches\")\n",
                "    report.append(f\"   4. Convert date columns to proper datetime format\")\n",
                "    report.append(f\"   5. Create derived features for business analysis\")\n",
                "    \n",
                "    return \"\\n\".join(report)\n",
                "\n",
                "# Generate and display the report\n",
                "quality_report = generate_data_quality_report(datasets, missing_df, duplicates_df, relationships)\n",
                "print(quality_report)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save the data quality report to a file\n",
                "with open('../reports/data_quality_report.txt', 'w', encoding='utf-8') as f:\n",
                "    f.write(quality_report)\n",
                "\n",
                "print(\"\\nâœ… Data quality report saved to 'reports/data_quality_report.txt'\")\n",
                "print(\"\\nðŸŽ¯ NEXT STEPS:\")\n",
                "print(\"   1. Proceed with data cleaning based on identified issues\")\n",
                "print(\"   2. Handle missing values using appropriate strategies\")\n",
                "print(\"   3. Remove duplicates and resolve foreign key issues\")\n",
                "print(\"   4. Convert date columns and create derived features\")\n",
                "print(\"   5. Begin business analysis once data is clean\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}